{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iItkhsGB5c4W"
   },
   "source": [
    "# Lab4. Classification\n",
    "## Classification\n",
    "+ Logistic Regression\n",
    "+ SVM\n",
    "+ Decision Tree\n",
    "+ Random Forest\n",
    "\n",
    "## Evaluation\n",
    "+ Accuracy\n",
    "+ Confution Matrix\n",
    "+ ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAfXTYpj5ysB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 회귀 실습을 위해 sklearn 내장 데이터인 붓꽃 데이터를 불러온다.\n",
    "- 붓꽃 데이터셋은 꽃 받침의 길이, 너비, 꽃잎의 길이, 너비인 4개의 변수를 가지고 있으며, 3개의 붓꽃 종을 label data로 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iris.data\n",
    "label = iris.target\n",
    "columns = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns = columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "- Machine Learning과 통계학에서의 분류는 새로 관측된 데이터가 어떤 범주 집합에 속하는지를 식별하는 것을 말한다.\n",
    "- 훈련 데이터를 이용해 모델을 학습하면, 모델은 결정 경계(Decision boundary)라는 데이터를 분류하는 선을 만들어 낸다.\n",
    "- 이번 Lab에서는 여러가지 대표적인 모델의 원리를 간단히 알아보고, 결정 경계를 만들어 데이터를 분류해 본다.\n",
    "\n",
     " ![./Images/Classification.png](./img/Classification.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "- Regression 이라는 말에서 알 수 있듯이, Logistic 회귀 모델은 선형 회귀 모델에서 변형된 모델이다.\n",
    "- 출력변수가 두 가지(0이나 1)인 data를 자연스럽게 Modeling하고자, 0과 1이 나타나는 확률값이 출력되도록 선형회귀 모델을 확장한 것.\n",
    "- 예를 들면, 종양의 크기에 따라 악성 종양인지(Yes =1) 그렇지 않은지(No = 0)를 회귀를 이용해서 1과 0으로 예측하는 것이다.\n",
    "- 이 방법을 선형회귀로 하면 오차가 너무 많이 나와서 정확도가 떨어지기 때문에 Sigmoid 함수를 이용하면 좀 더 정확하게 1과 0에 대해 분류할 수 있다.\n",
    "- 따라서 Logistic Regression은 이처럼 Linear Regression 방식을 기반으로 하되, Sigmoid 함수를 이용해서 분류를 수행하는 Regression이다.\n",
    "- 추정은 제곱오차를 최소화하지 않고, 우도를 최대화(최대가능도방법, 최대우도법, Maximum Likelihood Method)해서 구한다.\n",
    "- Odds라는 어떤 일이 발생할 상대적인 비율 개념을 사용해 선형 회귀식을 변형한다.\n",
    "\n",
    "$$ Odds = {p \\over {1-p}} $$\n",
    "$$ p : 어떤\\ 일이\\ 발생할\\ 확률 $$\n",
    "\n",
    "- Odds를 그대로 사용하지말고 log를 취해 사용하면 0을 기준으로 상호 대칭적이 되며, 계산을 수월하게 할 수 있도록 변경해준다.\n",
    "- 기존의 선형 회귀식에서 y 위치에 log Odds를 적용하면 다음과 같은 식이 된다.\n",
    "\n",
    "$$ ln({Y \\over {1-Y}}) = wx + b $$\n",
    "\n",
    " ![./Images/logodd.png](./img/logodd.png)\n",
    "\n",
    "- 이를 y에 대해 정리하면 그 유명한 sigmoid 식이 된다.\n",
    "\n",
    "$$ y = {1 \\over {1+\\exp^{-(wx + b)}}} $$\n",
    "\n",
    "<img src='img/sigmoid.png' alt='sigmoid' width='400' height='550' /><br /><br />\n",
    "- Linear Regression은 잔차의 제곱을 최소화 하는 방식으로 학습을 했었다.\n",
    "- Logistic Regression은 Maximum Likelihood Estimation(MLE)이라는 과정을 통해 모델을 학습하는데, 자세한 내용은 아래 참조 목록에 있는 페이지를 확인할 것.\n",
    "- Logistic Regression은 종속변수를 직접 모델링하지 않고, 종속변수가 특정 범주에 속하는 확률을 Modeling한다.\n",
    "- 이 함수는 연속형인 값을 입력받아 0과 1 사이의 값으로 변환한다.\n",
    "- 이 변환과정을 Logit 변환이라고 한다.\n",
    "- Logit은 Odds에 log를 씌우는 것을 말한다.\n",
    "- 이산형 종속변수의 확률을 이용하여 오즈비(사건이 일어날 확률 / 사건이 일어나지 않을 확률)을 구한 뒤 log를 취하면 이산형 종속변수에 대한 예측이 가능해 진다.\n",
    "\n",
    "### Logistic 회귀은 이진 분류 모델로 알고 있는데, 어떻게 여러개의 클래스를 분류할 수 있는가?\n",
    "- 하나의 수식이 출력하는 결과는 클래스의 확률을 나타내는 것은 맞다. \n",
    "- 하지만, 멀티 클래스인 경우 내부적으로 클래스 수에 맞게 여러개의 수식을 만들어 각각의 클래스에 속할 확률을 계산한 후 가장 높은 확률은 가진 클래스로 분류한다.\n",
    "- 이를 One-vs-Rest라고 한다. \n",
    "- 자세한 내용은 참조 목록에 있는 페이지를 확인할 것.\n",
    "\n",
    "- Logistic Regression은 Sklearn의 linear_model package에 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, \n",
    "                                                    test_size=0.2, shuffle=True, stratify=label, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 결과처럼 label의 값은 0, 1, 2이다.\n",
    "- 이 값들을 적절하게 섞는 것이 shuffle = True이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 결과 예측하기 (테스트 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "- 일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('로지스틱 회귀, 정확도 : {:.2f}%'.format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model의 계수 w, 절편 b 살펴보기\n",
    "- 어떤 변수에 얼마 만큼의 가중치가 할당되고, 절편 값은 얼마나 할당되는지 살펴볼 수 있다.\n",
    "- 3개의 Multi Class 분류이므로 One-vs-Rest, 3개의 회귀식을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('로지스틱 회귀, 계수(w) : {}, 절편(b) : {}'.format(lr.coef_, lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이제 위스콘신 유방암 Dataset로 학습해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cancer = load_breast_cancer()   # Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(cancer.data)\n",
    "\n",
    "#Data를 학습 Dataset와 Test Dataset으로 나눈다.\n",
    "X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 로지스틱 회귀를 이용하여 학습 및 예측 수행. \n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_preds = lr_clf.predict(X_test)\n",
    "\n",
    "# accuracy와 roc_auc 측정\n",
    "print('accuracy: {:0.3f}'.format(accuracy_score(y_test, lr_preds)))\n",
    "print('roc_auc: {:0.3f}'.format(roc_auc_score(y_test , lr_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machine\n",
    "- Support Vector Machine(SVM, 서포트 벡터 머신)는 주어진 데이터를 바탕으로하여 두 카테고리(이진 분류의 경우) 사이의 간격(Margin, 마진)을 최대화하는 데이터 포인트(Support Vector, 서포트 벡터)를 찾아내고\n",
    "- 그 Support Vector에 수직인 경계를 통해 데이터를 분류하는 Algorithm이다.<br />\n",
    "<img src=\"./img/SVM1.png\" alt=\"Support Vector machine1\" style=\"height: 400px\" align='center'/>\n",
    "<div>\n",
    "    <img src=\"./img/SVM.png\" alt=\"Support Vector machine2\" style=\"height: 400px\" align='left'/>\n",
    "    <img src=\"./img/SVM2.png\" alt=\"Support Vector machine3\" style=\"height: 400px\" align='right'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왜 Margin을 최대화 할까?\n",
    "- SVN에서 나오는 마진은 물건을 판매할때 마진이 20%다 라고 말하는 그 마진이 맞다.\n",
    "- 그렇다면 경계면과의 마진을 최대화 하는 것이 왜 분류를 잘하는 것이라고 말하는 것인가?\n",
    "\n",
    "#### 경험적 위험 최소화(Empirical Risk Minimization, ERM) vs 구조적 위험 최소화(Structural Risk Minimization,SRM)\n",
    "* 경험적 위험 최소화 \n",
    "    * 훈련 데이터에 대해 위험을 최소화\n",
    "    * 학습 알고리즘의 목표\n",
    "    * 뉴럴 네트워크, 결정 트리, 선형 회귀, 로지스틱 회귀 등.\n",
    "* 구조적 위험 최소화\n",
    "    * 관찰하지 않은(Unseen) 데이터에 대해서도 위험을 최소화\n",
    "    * 오차 최소화를 일반화 시키는 것\n",
    "    * SVM\n",
    "    \n",
    "<div align='center'> \n",
    "    <font size=\"6\">어떤 모델이 더 좋을까?</font> \n",
    "</div>\n",
    "\n",
    "\n",
    "<img src=\"./img/ERM_SRM.png\" alt=\"ERM_SRM\" style=\"height: 300px\" align='center'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost : Soft or Hard\n",
    "- SVM에는 Soft Margin, Hard Margin 이라는 말이 있다. \n",
    "- 단어 자체에서도 유추할 수 있지만, Soft Margin은 유연한 경계면을 만들어내고 Hard Margin은 분명하게 나누는 경계면을 만들어낸다.\n",
    "- 그렇다면 왜 Soft Margin이 필요한가?\n",
    "\n",
     " ![./Images/Softmargin.png](./img/Softmargin.png)\n",
    "\n",
    "- 다음과 같은 데이터 분포는 직선으로 두개의 데이터를 나누는 경계면을 만들기 어렵다. \n",
    "- 현실에서도 우리가 최적의 답을 찾지 못할때 어느정도 비용(Cost, C)을 감수하면서 적절한 답을 찾는 것을 떠올려보자.\n",
    "- Soft Margin은 그런 원리이다.\n",
    "- 경계면을 조금씩 넘어가는 데이터들(비용, Cost)을 감수하면서 가장 차선의 경계면을 찾는다.\n",
    "- 실제 알고리즘에서도 C(Cost)값을 통해 얼마나 비용을 감수할 것인지를 결정할 수 있다. \n",
    "- 크면 클수록 Hard Margin을, 작으면 작을수록 Soft Margin을 만들어낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저차원을 고차원으로 Kernel Trick\n",
    "- SVM은 기본적으로 선형 분류를 위한 경계면을 만들어낸다. \n",
    "- 그렇다면 어떻게 비선형 분류를 할 수 있을까?\n",
    "\n",
    "<img src = \"./img/Hyperplane.png\" alt=\"hyper\" style=\"height: 300px\" />\n",
    "\n",
    "- 저차원(2차원)에서는 선형 분리가 되지 않을 수 있지만, 고차원(3차원)에서는 선형 분리가 가능할 수 있다.\n",
    "- 이러한 원리를 바탕으로 선형 분리가 불가능한 저차원 데이터를 선형 분리가 가능한 어떤 고차원으로 보내 선형 분리를 할 수 있다.\n",
    "- 하지만, 저차원 데이터를 고차원으로 보내서 Support Vector를 구하고 저차원으로 내리는 과정에서 더 복잡해지고 연산량도 많아질것이 분명하다. \n",
    "- 그래서 여기에서 Kernel Trick이라는 Mapping 함수를 사용한다.\n",
    "- Kernel Trick은 고차원 Mapping과 고차원에서의 내적 연산을 한번에 할 수 있는 방법이다.\n",
    "- 이를 통해 여러가지 Kernel 함수를 통해 저차원에서 해결하지 못한 선형 분리를 고차원에서 해결할 수 있다.\n",
    "\n",
    "대표적인 Kernel 함수\n",
    "- Linear (선형 함수)\n",
    "- Poly   (다항식 함수)\n",
    "- RBF    (방사기저 함수)\n",
    "- Hyper-Tangent (쌍곡선 탄젠트 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM 분류기는 Sklearn의 svm 패키지에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 iris Dataset을 불러온다.\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "data = iris.data\n",
    "label = iris.target\n",
    "columns = iris.feature_names\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, \n",
    "                                                    test_size=0.2, shuffle=True, stratify=label, random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 결과 예측하기 (테스트 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "- 일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('서포트 벡터 머신, 정확도 : {:.2f}%'.format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# BMI를 계산해서 레이블을 리턴하는 함수\n",
    "def calc_bmi(h, w):\n",
    "    bmi = w / (h/100) ** 2\n",
    "    if bmi < 18.5: return \"thin\"\n",
    "    if bmi < 25: return \"normal\"\n",
    "    return \"fat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 파일 준비하기\n",
    "fp = open(\"bmi.csv\",\"w\", encoding=\"utf-8\")\n",
    "fp.write(\"height,weight,label\\r\\n\")\n",
    "\n",
    "# 무작위로 데이터 생성하기\n",
    "cnt = {\"thin\":0, \"normal\":0, \"fat\":0}\n",
    "for i in range(20000):\n",
    "    h = random.randint(120,200)\n",
    "    w = random.randint(35, 80)\n",
    "    label = calc_bmi(h, w)\n",
    "    cnt[label] += 1\n",
    "    fp.write(\"{0},{1},{2}\\r\\n\".format(h, w, label))\n",
    "fp.close()\n",
    "print(\"ok,\", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 키와 몸무게 데이터 읽어 들이기 --- (step 1)\n",
    "tbl = pd.read_csv(\"bmi.csv\")\n",
    "\n",
    "# 칼럼(열)을 자르고 정규화하기 --- (step 2)\n",
    "label = tbl[\"label\"]\n",
    "w = tbl[\"weight\"] / 100 # 최대 100kg라고 가정\n",
    "h = tbl[\"height\"] / 200 # 최대 200cm라고 가정\n",
    "wh = pd.concat([w, h], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기 --- (step 3)\n",
    "data_train, data_test, label_train, label_test = train_test_split(wh, label)\n",
    "\n",
    "# 데이터 학습하기 --- (step 4)\n",
    "clf = svm.SVC()\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "# 데이터 예측하기 --- (step 5)\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "# 결과 테스트하기 --- (step 6)\n",
    "ac_score = metrics.accuracy_score(label_test, predict)\n",
    "cl_report = metrics.classification_report(label_test, predict)\n",
    "print(\"정답률 =\", ac_score)\n",
    "print(\"리포트 =\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이처럼 BMI 공식을 모르는 상황에서도 키, 몸무게, 레이블만 있으면 Machine Learning으로 저체중(thin), 정상(normal), 비만(fat)을 구분할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision Tree\n",
    "- 이전의 회귀 수업에서 결정 트리에 대해 간단하게 살펴보았다.\n",
    "- 결정 트리는 입력 변수를 특정한 기준으로 잘라(분기) Tree 형태의 구조로 분류를 하는 모델이다.\n",
    "- 결정 트리는 ML Algorithm 중 직관적으로 이해하기 쉬운 Algorithm이다.\n",
    "- Data에 있는 규칙을 학습을 통해 자동으로 찾아내 Tree 기반의 분류 규칙을 만드는 것이다.\n",
    "- 일반적으로 규칙을 가장 쉽게 표현하는 방법은 if/else 기반으로 나타내는 것이다.\n",
    "- 예를 들면, 스무고개 게임과 유사.\n",
    "- 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 Algorithm의 성능을 크게 좌우한다.\n",
    "<img src='./img/DecisionTree.png' style=\"height: 400px\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/DecisionTree1.png' style=\"height: 400px\"  />\n",
    "<img src='./img/DecisionTree2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/DTDesc.png' style=\"height: 400px\"  />\n",
    "<br>\n",
    "\n",
    "* 사람의 논리적 사고 방식을 모사하는 분류 방법론\n",
    "* IF-THEN rule의 조합으로 class 분류\n",
    "* 결과를 나무 모양으로 그릴 수 있음\n",
    "* Greedy 한 알고리즘 (한번 분기하면 이후에 최적의 트리 형태가 발견되더라도 되돌리지 않음, 최적의 트리 생성을 보장하지 않음)\n",
    "* 축에 직교하는 분기점\n",
    "* 데이터 전처리가 필요 없음\n",
    "\n",
    "<img src='./img/DT_G.png' style=\"height: 400px\"  />\n",
    "\n",
    "#### 불순도(Impurity, Entropy)\n",
    "- 결정 트리는 데이터의 불순도를 최소화 할 수 있는 방향으로 트리를 분기한다. \n",
    "- 불순도란 정보 이론(Information Theory)에서 말하는 얻을 수 있는 정보량이 많은 정도를 뜻한다. \n",
    "- Entropy : 얼마만큼의 정보를 담고 있는가?\n",
    "- ex) 오늘 해가 동쪽에서 뜰꺼야 -> 낮은 정보량, 오늘 일식이 일어날꺼야 -> 높은 정보량\n",
    "- 정보 이론의 정보량의 자세한 내용은 참조 목록에 링크로 있다. \n",
    "- 확인해보면 좋겠다.\n",
    "\n",
    "- 결정 트리 모델은 Sklearn의 tree package에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 결과 예측하기 (테스트 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "- 일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('결정 트리, 정확도 : {:.2f}%'.format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "- Tree 기반 모델은 트리를 분기하는 과정에서 어떤 변수가 모델을 생성하는데 중요한지에 대한 변수 중요도를 살펴볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(dt.feature_importances_.reshape((1, -1)), columns=columns, index=['feature_importance'])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UCI Machine Learning Repository(http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)에서 제공하는 사용자 행동인식(Human Activity Recognition) Dataset으로 예측 분류를 한다.\n",
    "- 해당 data는 30명에게 Smart Phone Sensor를 장착한 뒤 사람의 동작과 관련된 여러 가지 feature를 수집한 data이다.\n",
    "- 수집된 feature set를 기반으로 Dicision Tree를 이용해 어떠한 동작인지 예측해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 해당 site(http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)에 접속한다.\n",
    "2. [Data Folder] link(http://archive.ics.uci.edu/ml/machine-learning-databases/00240/)를 click한다.\n",
    "3. 해당 Dataset를 내려받을 수 있는 archive list가 나타난다.\n",
    "4. 여기서 [UCI HAR Dataset.zip]을 click해서 download한다.\n",
    "5. Download받은 파일의 압축을 풀면, folder name을 'human_activity'로 변경한다.\n",
    "6. 'human_activity' folder 속에 'features_info.txt'을 열어보면 간략한 설명이 되어있다.\n",
    "7. train folder와 test folder에 실제 data가 들어있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.\n",
    "feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\\s+',\n",
    "                        header=None,names=['column_index','column_name'])\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "print('전체 피처명에서 10개만 추출:', feature_name[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_human_dataset( ):\n",
    "    \n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n",
    "    feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\\s+',\n",
    "                        header=None,names=['column_index','column_name'])\n",
    "    \n",
    "    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('./human_activity/train/X_train.txt',sep='\\s+', names=feature_name)\n",
    "    X_test = pd.read_csv('./human_activity/test/X_test.txt',sep='\\s+', names=feature_name)\n",
    "    \n",
    "    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n",
    "    y_train = pd.read_csv('./human_activity/train/y_train.txt',sep='\\s+',header=None,names=['action'])\n",
    "    y_test = pd.read_csv('./human_activity/test/y_test.txt',sep='\\s+',header=None,names=['action'])\n",
    "    \n",
    "    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('## 학습 피처 데이터셋 info()')\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train['action'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "dt_clf.fit(X_train , y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test , pred)\n",
    "print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier의 하이퍼 파라미터 추출\n",
    "print('DecisionTreeClassifier 기본 하이퍼 파라미터:\\n', dt_clf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이번에는 결정 트리의 트리 깊이(Tree depth)가 예측 정확도에 주는 영향을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth' : [ 6, 8 ,10, 12, 16 ,20, 24]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )\n",
    "grid_cv.fit(X_train , y_train)\n",
    "print('GridSearchCV 최고 평균 정확도 수치:{0:.4f}'.format(grid_cv.best_score_))\n",
    "print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV객체의 cv_results_ 속성을 DataFrame으로 생성. \n",
    "cv_results_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "cv_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출\n",
    "cv_results_df[['param_max_depth', 'mean_test_score', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [ 6, 8 ,10, 12, 16 ,20, 24]\n",
    "# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정\n",
    "for depth in max_depths:\n",
    "    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)\n",
    "    dt_clf.fit(X_train , y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    print('max_depth = {0} 정확도: {1:.4f}'.format(depth , accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth' : [ 8 , 12, 16 ,20], \n",
    "    'min_samples_split' : [16,24],\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )\n",
    "grid_cv.fit(X_train , y_train)\n",
    "print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))\n",
    "print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df_clf = grid_cv.best_estimator_\n",
    "pred1 = best_df_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test , pred1)\n",
    "print('결정 트리 예측 정확도:{0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "- 결정 트리가 나무였다면, Random Forest는 숲이다. \n",
    "- Random Forest의 특징은 작은 Tree들을 여러개 만들어 합치는 모델이다.\n",
    "- 서로 다른 변수 셋으로 여러 트리를 생성한다. \n",
    "- 여러개의 모델을 합치는 Ensemble 기법 중 대표적인 예시이다.\n",
    "- 학습 전용 데이터를 기반으로 다수의 의사결정 트리를 만들고, 만들어진 의사결정 트리를 기반으로 다수결(투표 또는 평균 등)로 결과를 유도하므로 높은 정밀도를 자랑한다.\n",
    "\n",
    "<img src='img/randomforest.png' alt='Random Forest' />\n",
    "\n",
    "#### 위의 예제와 같이 Random Forest 모델을 사용해보자.\n",
    "- Random Forest Model은 Sklearn의 ensemble package에 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "data = iris.data\n",
    "label = iris.target\n",
    "columns = iris.feature_names\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, \n",
    "                                                    test_size=0.2, shuffle=True, stratify=label, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=5,)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "print('랜덤 포레스트, 정확도 : {:.2f}%'.format(accuracy_score(y_test, y_pred)*100))\n",
    "feature_importance = pd.DataFrame(rf.feature_importances_.reshape((1, -1)), columns=columns, index=['feature_importance'])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 실습은 UCI Machine Learning Repository의 독버섯과 관련한 dataset를 사용하기로 한다.\n",
    "- https://archive.ics.uci.edu/ml/datasets/Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "local= \"mushroom.csv\"\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\"\n",
    "req.urlretrieve(url, local)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 쉼표로 구분되어 있고, 열은 23개이며\n",
    "- 가장 왼쪽의 열은 독의 유무이다.\n",
    "- 독이 있으면 p(poisionous), 식용이면 e(edible)이다.\n",
    "- 두번째 열은 버섯의 머리 모양이다.\n",
    "- 벨 형태(b), 원뿔 형태(c), 볼록한 형태(x), 평평한 형태(f), 혹 형태(k), 오목한 형태(s).\n",
    "- 네 번째 열은 버섯의 머리 색이다.\n",
    "- 갈색(n), 활갈색(b), 연한 갈색(c), 회색(g), 녹색(g), 분홍색(p), 보라색(u), 붉은색(e), 흰색(w), 노란색(y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 읽어 들이기--- (step1)\n",
    "mr = pd.read_csv(\"mushroom.csv\", header=None)\n",
    "\n",
    "# 데이터 내부의 기호를 숫자로 변환하기--- (step 2)\n",
    "label = []\n",
    "data = []\n",
    "attr_list = []\n",
    "for row_index, row in mr.iterrows():\n",
    "    label.append(row.ix[0])\n",
    "    row_data = []\n",
    "    for v in row.ix[1:]:\n",
    "        row_data.append(ord(v))\n",
    "    data.append(row_data)\n",
    "    \n",
    "# 학습 전용과 테스트 전용 데이터로 나누기 --- (step 3)\n",
    "data_train, data_test, label_train, label_test = train_test_split(data, label)\n",
    "\n",
    "# 데이터 학습시키기 --- (step 4)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "# 데이터 예측하기 --- (step 5)\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "# 결과 테스트하기 --- (step 6)\n",
    "ac_score = metrics.accuracy_score(label_test, predict)\n",
    "cl_report = metrics.classification_report(label_test, predict)\n",
    "print(\"정답률 =\", ac_score)\n",
    "print(\"리포트 =\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 읽어 들이기\n",
    "mr = pd.read_csv(\"mushroom.csv\", header=None)\n",
    "\n",
    "# 데이터 내부의 분류 변수 전개하기\n",
    "label = []\n",
    "data = []\n",
    "attr_list = []\n",
    "for row_index, row in mr.iterrows():\n",
    "    label.append(row.ix[0])\n",
    "    exdata = []\n",
    "    for col, v in enumerate(row.ix[1:]):\n",
    "        if row_index == 0:\n",
    "            attr = {\"dic\": {}, \"cnt\":0}\n",
    "            attr_list.append(attr)\n",
    "        else:\n",
    "            attr = attr_list[col]\n",
    "            \n",
    "        # 버섯의 특징 기호를 배열로 나타내기\n",
    "        d = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        if v in attr[\"dic\"]:\n",
    "            idx = attr[\"dic\"][v]\n",
    "        else:\n",
    "            idx = attr[\"cnt\"]\n",
    "            attr[\"dic\"][v] = idx\n",
    "            attr[\"cnt\"] += 1\n",
    "        d[idx] = 1\n",
    "        exdata += d\n",
    "    data.append(exdata)\n",
    "    \n",
    "# 학습 전용 데이터와 테스트 전용 데이터로 나누기\n",
    "data_train, data_test, label_train, label_test = train_test_split(data, label)\n",
    "\n",
    "# 데이터 학습시키기\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "# 데이터 예측하기\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "# 결과 테스트하기\n",
    "ac_score = metrics.accuracy_score(label_test, predict)\n",
    "\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "### 1. Accuracy, 정확도\n",
    "- 모든 데이터에 대해 클래스 라벨을 얼마나 잘 맞췄는지를 계산 \n",
    "\n",
     " ![./Images/Accuracy.png](./img/Accuracy.png)\n",
    "\n",
    "### 2. Confusion Matrix, 혼동 행렬\n",
    "- 정확도로는 분류 모델의 평가가 충분하지 않을 수 있다. \n",
    "- 예를 들어, 병이 있는 사람을 병이 없다고 판단하는 경우 Risk가 높기 때문에 모델의 목적에 맞게 분류 모델을 평가해야 한다. \n",
    "- 이때 사용되는 것이 Confusion Matrix 이다. \n",
    "\n",
     " ![./Images/Confusion_Matrix.png](./img/Confusion_Matrix.png)\n",
    "\n",
    "* Precision, 정밀도  : TP/(FP+TP), 1이라고 예측한 것 중 실제로 1인 것\n",
    "* Sensitivity, 민감도 : True Positive rate = Recall = Hit ratio = TP/(TP+FN), 실제로 1인 것 중에 1이라고 예측한 것\n",
    "* Specificity, 특이도 : True Negative rate = TN/(FP+TN), 실제로 0인 것 중에 0이라고 예측한 것 \n",
    "* False Alarm, 오탐 : False Positive rate = 1-Specificity = FP/(FP+TN), 실제로 0인 것 중에 1이라고 예측한 것\n",
    "\n",
    "### 3. ROC Curve, AUC\n",
    "- ROC Curve(Receiver-Operating Characteristic curve)는 민감도와 특이도가 서로 어떤 관계를 가지며 변하는지를 2차원 평면상에 표현한 것이다.\n",
    "- ROC Curve가 그려지는 곡선을 의미하고, AUC(Area Under Curve)는 ROC Curve의 면적을 뜻한다.\n",
    "- AUC 값이 1에 가까울 수록 좋은 모델을 의미한다.\n",
    "\n",
    " ![./Images/ROC_AUC.png](./img/ROC_AUC.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정확도, 정밀도, 민감도, AUC Score\n",
    "- 여러 Scoring 함수를 실습해 보기 위해 전복 데이터를 사용하려고 한다.\n",
    "- 그리고 정밀도, 민감도, AUC Score는 이진 분류인 경우에 사용할 수 있으므로, 수컷, 암컷 데이터만 학습하여 예측한 점수를 계산해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_path = join('sample_data', 'abalone.txt')\n",
    "column_path = join('sample_data', 'abalone_attributes.txt')\n",
    "\n",
    "abalone_columns = list()\n",
    "for l in open(column_path):\n",
    "    abalone_columns.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(abalone_path, header=None, names=abalone_columns)\n",
    "data.head()\n",
    "data = data[data['Sex'] != 'I']\n",
    "label = data['Sex'].map(lambda x : 0 if x == 'M' else 1)\n",
    "\n",
    "del data['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, label, stratify=label, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=5)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred =rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy : {:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Precision : {:.3f}'.format(precision_score(y_test, y_pred)))\n",
    "print('Recall : {:.3f}'.format(recall_score(y_test, y_pred)))\n",
    "print('AUC : {:.3f}'.format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- Wikipedia, Classification : https://en.wikipedia.org/wiki/Statistical_classification\n",
    "- Maximum Likelihood Estimation, 최대 우도 추정 : https://ratsgo.github.io/statistics/2017/09/23/MLE/\n",
    "- One-vs-Rest : https://datascienceschool.net/view-notebook/7a6b958e9d51451689138cca93a047d8/\n",
    "- Information Theory 설명 : https://ratsgo.github.io/statistics/2017/09/22/information/\n",
    "- ROC AUC 설명 : https://adnoctum.tistory.com/121\n",
    "- Sklearn, iris dataset : http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "- Sklearn, Logistic Regression : https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&sa=U&ved=0ahUKEwibhIa0-uDhAhVPeXAKHfPhCYQQFggEMAA&client=internal-uds-cse&cx=016639176250731907682:tjtqbvtvij0&usg=AOvVaw2AirAop04TUH9X2S1r9FVd\n",
    "- Sklearn, SVM : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- Sklearn, Decision Tree : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- Sklearn, Random Forest : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_04) Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
