{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxmD9kYf4CPV"
   },
   "source": [
    "# Lab. 군집화\n",
    "\n",
    "## Clustering\n",
    "+ k-means Clustering\n",
    "+ Hierarchical Clustering\n",
    "\n",
    "## Evaluation\n",
    "+ Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 군집화 실습을 위해 sklearn 내장 데이터인 와인 Dataset을 불러온다.\n",
    "- 와인 Dataset은 알콜, 말산, 페놀 등 13개의 변수를 가지고 있으며, 1,2,3 와인 등급을 label data로 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wine.data\n",
    "label = wine.target\n",
    "columns = wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns = columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIZRKFAJ4UQW"
   },
   "source": [
    "## Clustering\n",
    "- 주어진 데이터들의 특성을 고려해 Data Cluster를 정의하고, Cluster를 대표할 수 있는 대표점을 찾는 비지도 학습의 대표적인 Algorithm.\n",
    "- 간단히 말해서, 비슷한 특성을 가진 데이터끼리 묶는다고 말할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIZRKFAJ4UQW"
   },
   "source": [
    "### 1. k-Means Clustering\n",
    "- k-means Clustering은 대표적인 Clustering Algorithm 중 하나로, 각 Cluster에 할당된 Data Point들의 평균 좌표를 이용해 군집중심점(centroid)을 반복적으로 update하며 Cluster를 형성하는 Algorithm이다.\n",
    "- 군집 중심점은 선택된 point의 평균 지점으로 이동하고 이동된 중심점에서 다시 가까운 point를 선택, 다시 중심점을 평균 지점으로 이동하는 process를 반복적으로 수행한다.\n",
    "- 모든 data point에서 더 이상 중심점의 이동이 없을 경우에 반복을 멈추고 해당 중심점에 속하는 data point들을 군집화하는 기법이다.\n",
    "\n",
    " ![./Images/k-means-clustering.png](./img/k-means-clustering.png)\n",
    "- k-means Clustering Algorithm은 3가지 단계로 이루어진다.<br />\n",
    "Step 1. 각 Data Point i 에 대해 가장 가까운 중심점을 찾고, 그 중심점에 해당하는 Cluster를 할당.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;가까운 중심점을 찾을 때는, **유클리드 거리** 를 사용.<br>\n",
    "Step 2. 할당된 Cluster를 기반으로 새로운 중심점을 계산.<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;중심점은 Cluster 내부 점들 좌표의 **산술 평균(mean)** 으로 한다.<br>\n",
    "Step 3. 각 Cluster의 할당이 바뀌지 않을 때까지 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 점과 점사이의 거리를 어떻게 측정할 수 있을까? \n",
    "\n",
    " k-means Clustering은 거리 기반 Algorithm이므로 여러가지 방법으로 거리를 측정할 수 있다.<br>\n",
    " #### 1. Manhattan Distance - 각 축에 대해 수직으로만 이동하여 계산하는 거리 측정방식\n",
    " $$D(x,y) = {{\\sum_{i=1}^{d}  |x_i - y_i|} } $$\n",
    " ![./Images/Manhattan.png](./img/Manhattan.png)\n",
    " \n",
    " #### 2. Euclidean Distance - 점과 점사이의 가장 짧은 거리를 계산하는 거리 측정방식\n",
    " $$D(x,y) = {\\sqrt{\\sum_{i=1}^{d}  (x_i - y_i)^2} } $$\n",
    " ![./Images/Euclidean.png](./img/Euclidean.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 준비\n",
    "- wine Data는 13개의 column을 가지고 있고, 하나의 데이터(행)는 13개의 차원으로 이루어진 vector라고 볼 수 있다. \n",
    "- 13차원은 우리 눈으로 볼 수 있도록 표현하기 어려우므로 앞에서 배운 pca를 통해 2차원으로 만들어 시각화할 수 있도록 변환하자.\n",
    "- 그 전에 각 변수들의 값의 범위가 서로 다르므로 min-max 정규화를 통해 조정해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "data = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Model 불러오기 및 정의하기\n",
    "- Clustering은 비지도학습이므로 Cluster의 수는 label의 수와 관계 없지만, 3개의 군집을 형성하도록 한다.\n",
    "- k-means Clustering은 sklearn의 cluster package에 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Model 학습하기 (Clustering을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Cluster 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = kmeans.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], c=cluster, linewidth=1, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means를 이용한 붓꽃(Iris) 데이터 셋 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "iris = load_iris()\n",
    "# 보다 편리한 데이터 Handling을 위해 DataFrame으로 변환\n",
    "irisDF = pd.DataFrame(data=iris.data, columns=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "irisDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300,random_state=0).fit(irisDF)\n",
    "# init : 초기에 군집 중심점의 좌표를 설정할 방식을 말하며, 보통은 임의로 중심을 설정하지 않고 일반적으로 k-means++ 방식으로 최초 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDF['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDF['target'] = iris.target\n",
    "iris_result = irisDF.groupby(['target','cluster'])['sepal_length'].count()\n",
    "print(iris_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_transformed = pca.fit_transform(iris.data)\n",
    "\n",
    "irisDF['pca_x'] = pca_transformed[:,0]\n",
    "irisDF['pca_y'] = pca_transformed[:,1]\n",
    "irisDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster 값이 0, 1, 2 인 경우마다 별도의 Index로 추출\n",
    "marker0_ind = irisDF[irisDF['cluster']==0].index\n",
    "marker1_ind = irisDF[irisDF['cluster']==1].index\n",
    "marker2_ind = irisDF[irisDF['cluster']==2].index\n",
    "\n",
    "# cluster값 0, 1, 2에 해당하는 Index로 각 cluster 레벨의 pca_x, pca_y 값 추출. o, s, ^ 로 marker 표시\n",
    "plt.scatter(x=irisDF.loc[marker0_ind,'pca_x'], y=irisDF.loc[marker0_ind,'pca_y'], marker='o') \n",
    "plt.scatter(x=irisDF.loc[marker1_ind,'pca_x'], y=irisDF.loc[marker1_ind,'pca_y'], marker='s')\n",
    "plt.scatter(x=irisDF.loc[marker2_ind,'pca_x'], y=irisDF.loc[marker2_ind,'pca_y'], marker='^')\n",
    "\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('3 Clusters Visualization by 2 PCA Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means의 장점\n",
    "- 일반적인 군집화에서 가장 많이 활용되는 Algorithm이다.\n",
    "- Algorithm이 쉽고 간결하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means의 단점\n",
    "- 거리 기반 Algorithm으로 속성의 갯수가 매우 많을 경우 군집화 정확도가 떨어진다. 그래서 PCA로 차원 감소를 적용해야 할 수 있다.\n",
    "- 반복을 수행하는데, 반복 횟수가 많을 경우 수행 시간이 매우 느려진다.\n",
    "- 몇 개의 군집(Cluster)을 선택해야 할지 가이드하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering\n",
    "- 거리(Distance) 또는 유사도(Similarity)를 기반으로 Cluster를 형성하는 Algorithm 이다. \n",
    "- k-means Clustering과 다르게 Cluster의 수를 설정해 줄 필요가 없으며, Cluster 형태를 시각적으로 표현해주는 Dendrogram을 통해 적절한 Cluster의 수를 선택할 수 있다.\n",
    "- Dendrogram : 각 단계에서 관측치의 군집화를 통해 형성된 group과 이들의 유사성 수준을 표시하는 Tree Diagram.\n",
    "- Hierachichal Clustering에는 Bottom-Up 방식의 Agglomerative Method와 Top-Down 방식의 Divisive Method로 나뉜다.<br /><br />\n",
    "여기에서는 Agglomerative Method를 사용해 실습을 진행한다.\n",
    "<br><br>Agglomerative Method를 사용한 Hierarchical Clustering Algorithm은 3가지 단계로 이루어진다.<br>\n",
    "- Step 1. 각 Data Point를 Cluster로 할당한다. (n개의 Cluster)\n",
    "- Step 2. 가까운 Cluster끼리 병합한다.\n",
    "- Step 3. 1개의 Cluster가 될 때까지 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떻게 가장 가까운 클러스터를 찾을 수 있을까?\n",
    "- 방금전 거리 측정 방법으로 맨하탄 거리, 유클리디언 거리에 대해 배웠었다.\n",
    "- k-means에서는 각 Cluster의 중심점 간의 거리로 Cluster간 거리를 계산했었다.\n",
    "\n",
    "<br />이번 수업에서는 새로운 Cluster간 거리를 계산하는 방법에 대해 알아보도록 한다.<br>\n",
    "#### 1. Single Linkage - 두 Cluster 내의 가장 가까운 점 사이의 거리 \n",
    "![Single Linkage](./img/Single.png)\n",
    "#### 2. Complete Linkage - 두 Cluster 내의 가장 먼 점 사이의 거리\n",
    "![Complete Linkage](./img/Complete.png)\n",
    "#### 3. Average Linkage - 두 Cluster 내의 모든 점 사이의 평균 거리\n",
    "![Average Linkage](./img/Average.png)\n",
    "\n",
    "#### 3개 거리 측정 방식의 결과와 차이점을 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "single_clustering = AgglomerativeClustering(n_clusters=3, linkage='single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (클러스터링을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_clustering.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 클러스터 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_cluster = single_clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "* 산점도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=single_cluster)\n",
    "plt.title('Sklearn Single Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = single_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 한다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 Cluster 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# Dendrogram을 그리기위한 연결 Metrix 생성.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# Dendrogram을 그린다.\n",
    "dendrogram(linkage_matrix, p = len(data), labels = single_cluster, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Model 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_clustering = AgglomerativeClustering(n_clusters=3, linkage='complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Model 학습하기 (Clustering을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_clustering.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 클러스터 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_cluster = complete_clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "* 산점도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=complete_cluster)\n",
    "plt.title('Sklearn Complete Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = complete_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 한다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 Cluster 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# Dendrogram을 그리기위한 연결 Matrix를 생성합니다.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# Dendrogram을 그립니다.\n",
    "dendrogram(linkage_matrix, p = len(data), labels = complete_cluster, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Model 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_clustering = AgglomerativeClustering(n_clusters=3, linkage='average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Model 학습하기 (Clustering을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_clustering.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Clustering 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cluster = average_clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "* 산점도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=average_cluster)\n",
    "plt.title('Sklearn Average Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = average_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 한다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 Cluster 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# Dendrogram을 그리기위한 연결 Matrix 생성.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# Dendrogram을 그린다.\n",
    "dendrogram(linkage_matrix, p = len(data), labels = average_cluster, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering 결과 비교하기\n",
    "1. Single Linkage\n",
    "    + 두 Cluster 내의 가장 가까운 점을 기준으로 Cluster를 합치기 때문에 Cluster 사이의 Noise에 매우 민감한 특성과 구 형태가 아닌 data에 대해 Cluster를 잘 형성한다는 특성이 있다.\n",
    "    + wine data는 모든 data가 연결되어 있는 듯한 분포를 가지고 있기 때문에, 각 Cluster의 경계가 모호한 Noise가 많은 형태를 띠고 있다. <br>Single Linkage가 구 형태가 아닌 data에 대해 Cluster를 잘 형성한다는 특성이 있지만, 이러한 data의 경우 Single Linkage 방법을 사용하면 좋은 Cluster를 생성하기 어렵다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=single_cluster)\n",
    "plt.title('Sklearn Single Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete Linkage\n",
    "    + 두 Cluster 내에 가장 먼 점을 기준으로 Cluster를 합치기 때문에 Cluster 사이의 Noise와 이상치에 민감하지 않은 특성이 있다.\n",
    "    + Noise에 민감하지 않다는 특성을 가진 Complete Linkage가 좋은 성능을 보여주었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Average Linkage\n",
    "    + Single Linkage와 Complete Linkage의 중간쯤에 위치한 Average Linkage가 가장 정답에 가까운 Cluster를 형성한 것을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data[:,0], data[:,1], c=complete_cluster)\n",
    "plt.title('Sklearn Complete Linkage Hierarchical Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(data[:,0], data[:,1], c=average_cluster)\n",
    "plt.title('Sklearn Average Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Silhouette\n",
    "+ Silhouette 값은 한 cluster 안의 데이터들이 다른 cluster와 비교해서 얼마나 비슷한가를 나타냅니다.\n",
    "+ 다시 말해서 각 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지를 나타낸다.\n",
    "+ 같은 cluster 내의 점들간 거리는 가깝고(cohesion) 서로 다른 cluster 간의 거리는 멀수록(separation) 높은 값을 얻을 수 있다.\n",
    "+ Silhouette 값이 1에 근접한다는 것은 같은 cluster 내의 평균거리가 다른 cluster와의 평균거리보다 가깝다는 것을 의미한다.\n",
    "+ 일반적으로 Silhouette 값이 0.5보다 크다면 데이터가 잘 clustering 되었다는 것을 나타낸다.\n",
    "\n",
    "Silhouette 공식은 다음과 같다.\n",
    "$$ S_i = { {(b_i - a_i)} \\over max(a_i, b_i) }$$\n",
    "\n",
    "$$ a_i\\ :\\ 같은\\ 클러스터\\ 내의\\ 모든\\ 점들\\ 간\\ 평균\\ 거리 $$\n",
    "$$ b_i\\ :\\ \\bar d\\ =\\ (i,c)의\\ 최솟값 $$\n",
    "$$ \\bar d\\ =\\ (i,c)\\ :\\ 다른\\ 클러스터\\ c와\\ i번째 데이터 와의\\ 평균\\ 거리$$\n",
    "<br>\n",
    "\n",
    "- 직관적으로 수식을 이해해보자.\n",
    "- a<sub>i</sub>는 같은 Cluster 내의 데이터 들이 잘 모여있다면 적은 값을 나타내고, b<sub>i</sub>는 각 Cluster들이 멀리 떨어져있다면 큰 값을 나타내게 된다.\n",
    "- 따라서 수식 S<sub>i</sub>에 따르면, 아주 잘 형성된(같은 Cluster는 가깝고 다른 Cluster끼리는 먼) Cluster 형태일 때 분모는 b<sub>i</sub>이 되고,<br> 분자는 b<sub>i</sub>에서 아주 작은 값인 a<sub>i</sub>가 빠져 1에 가까운 silhouette 값을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가장 좋은 Cluster를 형성하는 Cluster의 수를 찾아보자.\n",
    "- k-means Clustering과 Average Linkage를 사용한 Hierarchical Clustering에서 가장 높은 점수의 Cluster 수는 무엇인지 알아본다.\n",
    "\n",
    "- Silhouette Scoring은 Sklearn의 metrics 패키지에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n = 1\n",
    "best_score = -1\n",
    "\n",
    "for n_cluster in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=n_cluster)\n",
    "    kmeans.fit(data)\n",
    "    cluster = kmeans.predict(data)\n",
    "    score = silhouette_score(data, cluster)\n",
    "    \n",
    "    print('클러스터의 수 : {}, 실루엣 점수 : {:.2f}'.format(n_cluster, score))\n",
    "    if score > best_score :\n",
    "        best_n = n_cluster\n",
    "        best_score = score\n",
    "        \n",
    "print('가장 높은 실루엣 점수를 가진 클러스터 수 : {}, 실루엣 점수 : {:.2f}'.format(best_n, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Average Linkage Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "best_n = 1\n",
    "best_score = -1\n",
    "\n",
    "for n_cluster in range(2, 11):\n",
    "    average_clustering = AgglomerativeClustering(n_clusters= n_cluster, linkage='average')\n",
    "    average_clustering.fit(data)\n",
    "    cluster = average_clustering.labels_\n",
    "    score = silhouette_score(data, cluster)\n",
    "    \n",
    "    print('클러스터의 수 : {}, 실루엣 점수 : {:.2f}'.format(n_cluster, score))\n",
    "    if score > best_score :\n",
    "        best_n = n_cluster\n",
    "        best_score = score\n",
    "        \n",
    "print('가장 높은 실루엣 점수를 가진 클러스터 수 : {}, 실루엣 점수 : {:.2f}'.format(best_n, best_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- Wikipedia, Clustering : https://ko.wikipedia.org/wiki/클러스터_분석\n",
    "- Wikipedia, Manhattan distance : https://ko.wikipedia.org/wiki/맨해튼_거리\n",
    "- Wikipedia, Euclidean distance : https://ko.wikipedia.org/wiki/유클리드_거리\n",
    "- Sklearn, Wine dataset : https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html\n",
    "- Sklearn, k-Means Clustering : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- Sklearn, Hierarchical Clustering : https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html&sa=U&ved=0ahUKEwj_2aiGvt7hAhXLi7wKHei8CNsQFggEMAA&client=internal-uds-cse&cx=016639176250731907682:tjtqbvtvij0&usg=AOvVaw0zVZAVTxgORo-7LbgrNv_o\n",
    "- Sklearn, Silhouette : https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html&sa=U&ved=0ahUKEwi5lrTZwd7hAhUqCqYKHWCZCTEQFggEMAA&client=internal-uds-cse&cx=016639176250731907682:tjtqbvtvij0&usg=AOvVaw0-ZT8AJZRmR-qNpN-62Ei-\n",
    "- Sklearn, Silhouette Example : https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "- Scipy, Dendrogram : https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_02) Clustering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
