{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents() # 토큰화에 품사 태깅이 된 데이터 받아오기\n",
    "print(tagged_sentences[0]) # 첫번째 문장 샘플 출력\n",
    "print(\"품사 태깅이 된 문장 개수: \", len(tagged_sentences)) # 문장 샘플의 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentences, pos_tags =[], [] \n",
    "for tagged_sentence in tagged_sentences: # 3,914개의 문장 샘플을 1개씩 불러온다.\n",
    "    sentence, tag_info = zip(*tagged_sentence) \n",
    "    # 각 샘플에서 단어는 sentence에 품사 태깅 정보는 tags에 저장한다.\n",
    "    sentences.append(np.array(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n",
    "    pos_tags.append(np.array(tag_info)) # 각 샘플에서 품사 태깅 정보만 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0])\n",
    "print(pos_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[3])\n",
    "print(pos_tags[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.xlabel('length of Data')\n",
    "plt.ylabel('number of Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab=Counter()\n",
    "tag_set=set()\n",
    "\n",
    "for sentence in sentences: # 훈련 데이터 X에서 문장 샘플을 1개씩 꺼내온다.\n",
    "  for word in sentence: # 샘플에서 단어를 1개씩 꺼내온다.\n",
    "    vocab[word.lower()]=vocab[word.lower()]+1 # 각 단어의 빈도수를 카운트한다.\n",
    "\n",
    "for tags_list in pos_tags: # 훈련 데이터 y에서 품사 태깅 정보 샘플을 1개씩 꺼내온다.\n",
    "  for tag in tags_list: # 샘플에서 품사 태깅 정보를 1개씩 꺼내온다.\n",
    "    tag_set.add(tag) # 각 품사 태깅 정보에 대해서 중복을 허용하지 않고 집합을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab)) # X 데이터의 단어 집합의 길이 출력\n",
    "print(len(tag_set)) # y 데이터의 단어 집합의 길이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted=sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index={'PAD' : 0, 'OOV' :1}\n",
    "i=1\n",
    "# 인덱스 0은 각각 입력값들의 길이를 맞추기 위한 PAD(padding을 의미)라는 단어에 사용된다.\n",
    "# 인덱스 1은 모르는 단어를 의미하는 OOV라는 단어에 사용된다.\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    # if frequency > 1 :\n",
    "    # 빈도수가 1인 단어를 제거하는 것도 가능하겠지만 이번에는 별도 수행하지 않고 해보겠음.\n",
    "    # 참고로 제거를 수행할 경우 단어 집합의 크기가 절반 정도로 줄어듬.\n",
    "        i=i+1\n",
    "        word_to_index[word]=i\n",
    "print(word_to_index)\n",
    "print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_index={'PAD' : 0}\n",
    "i=0\n",
    "for tag in tag_set:\n",
    "    i=i+1\n",
    "    tag_to_index[tag]=i\n",
    "print(tag_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tag_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_index['UH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = []\n",
    "\n",
    "for s in sentences:\n",
    "    temp_X = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp_X.append(word_to_index.get(w.lower(),1))\n",
    "        except KeyError: \n",
    "            # 단어 집합을 만들 때 별도로 단어를 제거하지 않았기 때문에 \n",
    "            #이 과정에서는 OOV가 존재하지는 않음.\n",
    "            temp_X.append(word_to_index['OOV'])\n",
    "\n",
    "    data_X.append(temp_X)\n",
    "print(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word={}\n",
    "for key, value in word_to_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key\n",
    "\n",
    "\n",
    "temp = []\n",
    "for index in data_X[0] : # 첫번째 문장 샘플 안의 인덱스들에 대해서\n",
    "    temp.append(index_to_word[index]) # 다시 단어로 변환\n",
    "\n",
    "print(sentences[0]) # 기존 문장 샘플 출력 \n",
    "print(temp) # 기존 문장 샘플 → 정수 인코딩 → 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = []\n",
    "\n",
    "for s in pos_tags:\n",
    "    temp_y = []\n",
    "    for w in s:\n",
    "            temp_y.append(tag_to_index.get(w))\n",
    "\n",
    "    data_y.append(temp_y)\n",
    "\n",
    "print(data_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(len(l) for l in data_X)) # 모든 데이터에서 길이가 가장 긴 샘플의 길이 출력\n",
    "print(max(len(l) for l in data_y)) # 모든 데이터에서 길이가 가장 긴 샘플의 길이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=150\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "pad_X = pad_sequences(data_X, padding='post', maxlen=max_len)\n",
    "# data_X의 모든 샘플의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\n",
    "pad_y = pad_sequences(data_y, padding='post', value=tag_to_index['PAD'], maxlen=max_len)\n",
    "# data_y의 모든 샘플의 길이를 맞출 때 뒤의 공간에 'PAD'에 해당되는 인덱스로 채움.\n",
    "# 참고로 숫자 0으로 채우는 것과 'PAD'에 해당하는 인덱스로 채우는 것은 결국 0으로 채워지므로 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(len(l) for l in pad_X)) # 모든 데이터에서 길이가 가장 짧은 샘플의 길이 출력\n",
    "print(min(len(l) for l in pad_y)) # 모든 데이터에서 길이가 가장 짧은 샘플의 길이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(pad_X, pad_y, test_size=.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train2 = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "n_words = len(word_to_index)\n",
    "n_labels = len(tag_to_index)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words, 128, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(n_labels, activation=('softmax'))))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(0.001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train2, batch_size=128, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np_utils.to_categorical(y_test)\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "index_to_word={}\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "index_to_tag={}\n",
    "for key, value in tag_to_index.items():\n",
    "    index_to_tag[value] = key\n",
    "\n",
    "\n",
    "i=10 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = np.argmax(y_predicted, axis=-1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
    "true = np.argmax(y_test2[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
    "    if w != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_tag[t], index_to_tag[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
