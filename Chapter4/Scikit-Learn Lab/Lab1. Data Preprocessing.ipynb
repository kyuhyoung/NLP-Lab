{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EKBYrQlsAKS"
   },
   "source": [
    "# Lab. 데이터 전처리\n",
    "\n",
    "## Scaling\n",
    "+ Min-Max Normalize\n",
    "+ Standard Normalize(z-score)\n",
    "\n",
    "## Sampling\n",
    "+ Random Up-Down Sampling\n",
    "+ SMOTE\n",
    "\n",
    "## Dimensionality Reduction\n",
    "+ PCA\n",
    "\n",
    "## Categorical Variable to Numeric Variable\n",
    "+ Label Encoding\n",
    "+ One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data의 전처리(Data Preprocessing)\n",
    "- 데이터 전처리는 ML Algorithm 만큼 중요하다.\n",
    "- Scikit-learn의 ML Algorithm을 적용하기 전에 data에 대해 미리 처리해야 할 기본 사항이 있다.\n",
    "<br />\n",
    "- 결손값, 즉 NaN, Null 값은 허용되지 않는다.\n",
    "- 따라서 어떻게 해서든 Null 값은 고정된 다른 값으로 변환해야 한다.\n",
    "- 또한 Scikit-learn의 ML Algorithm은 문자열 값을 입력 값으로 허용하지 않는다.\n",
    "- 그래서 모든 문자열 값은 encodiing돼서 숫자 형으로 변환해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiHw3pBOnLBk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "abalone_path = join('sample_data', 'abalone.txt')\n",
    "column_path = join('sample_data', 'abalone_attributes.txt')\n",
    "\n",
    "abalone_columns = list()\n",
    "for l in open(column_path):\n",
    "    abalone_columns.append(l.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAPpXs6hoV0g"
   },
   "source": [
    "- 먼저 Machine Learning의 대표적인 Dataset 중 하나인 전복 data를 불러오자.\n",
    "- 전복 Dataset은 수컷, 암컷, 유아기 3개의 범주로 이루어진 범주형 변수와 길이, 직경, 높이, 무게 등 여러 수치형 변수로 이루어져 있다. \n",
    "- Data를 불러온 후 입력으로 사용할 변수들과 label로 사용할 성별 변수로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuBSDQ9inN-h"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(abalone_path, header=None, names=abalone_columns)\n",
    "label = data['Sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head() 함수로 처음 5개의 data를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1555418267315,
     "user": {
      "displayName": "강천성",
      "photoUrl": "",
      "userId": "18012903892325475989"
     },
     "user_tz": -540
    },
    "id": "e7KpY9HPJy6z",
    "outputId": "5e64305f-90b8-4de9-fbe4-89bf3b91f1b2"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrame에서 특정 컬럼을 제거하는 방법은 df.drop(컬럼이름) 또는 del df[컬럼이름] 을 사용해 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.describe() 함수로 각 변수별 평균, 표준편차, 최대, 최소, 사분위수 등의 기초 통계량을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.info() 함수로 각 변수들의 자료형을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61NiZOU7G4gR"
   },
   "source": [
    "# Scaling\n",
    "## Scaling은 왜 해야 하는가?\n",
    "- 변수의 크기가 너무 작거나, 너무 큰 경우 해당 변수가 Target 에 미치는 영향력이 제대로 표현되지 않을 수 있다.\n",
    "- Sklearn의 대표적인 Scaling 함수로는 특정 변수의 최대, 최소 값으로 조절하는 Min-Max Scaling과 z-정규화를 이용한 Standard Scaling이 있다.\n",
    "\n",
    "### 1. Min-Max Scaling\n",
    "- Min-Max Scaling을 하면, 값의 범위가 0 ~ 1 사이로 변경된다. \n",
    "- 수식을 직관적으로 이해해보면, X에 존재하는 어떤 가장 작은 값 x <sub>m</sub>에 대해서 x <sub>m</sub>는 Min(X)의 값과 같다.\n",
    "- 따라서 스케일링 후 x<sub>m</sub>은 0이되고, X에 존재하는 어떤 가장 큰 값x <sub>M</sub>은 분모의 식과 같아지므로 1이 된다.\n",
    "\n",
    "$$ x - Min(X) \\over Max(X) - Min(X) $$<br>\n",
    "$$X : 데이터\\ 셋 $$\n",
    "$$ x : 데이터\\ 샘플 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sklearn에서 Min-Max Scaler는 preprocessing 패키지에 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jnmJOwHnqu0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mMscaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터에서 특징 찾기(Min, Max 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAJfEDUnoJuc"
   },
   "outputs": [],
   "source": [
    "# mMscaler.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mMscaled_data = mMscaler.transform(data)\n",
    "mMscaled_data = mMscaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mMscaled_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mMscaled_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mMscaled_data = pd.DataFrame(mMscaled_data, columns = data.columns)\n",
    "mMscaled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Scaling\n",
    "+ z-score 라고 하는 Data를 통계적으로 표준 정규분포화시켜 Scaling을 하는 방식이다.<br>\n",
    "+ Data의 평균이 0, 표준 편차가 1이 되도록 Scaling 한다.\n",
    "\n",
    "$$ z = {{x - \\mu} \\over {\\sigma}} $$\n",
    "$$ \\mu : 데이터의\\ 평균, Mean(X) $$\n",
    "$$ \\sigma : 데이터의\\ 표준편차, Std(X)$$\n",
    "$$ X : 데이터\\ 셋 $$\n",
    "$$ x : 데이터\\ 샘플 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sklearn에서 Standard Scaler는 preprocessing 패키지에 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터에서 특징 찾기(Mean, Std 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdscaler.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdscaled_data = sdscaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdscaled_data = pd.DataFrame(sdscaled_data, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdscaled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 Data에서는 변수별로 서로 다른 평균과 표준 편차 값을 가지고 있었다.\n",
    "- Standard Scaling된 data를 살펴보면, 평균이 0 표준편차가 1이 되었음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "## Sampling은 왜 하는가?\n",
    "- Class 불균형 문제 : 분류를 목적으로하는 Dataset에 Class label의 비율이 균형을 맞추지 않고, 한쪽으로 치우친 경우를 말한다. \n",
    "- 이런 경우, Model이 각 Class의 Data를 제대로 학습하기 어려워진다.\n",
    "- 따라서 각 Class별 균형을 맞추는 작업이 필요한다.\n",
    "#### Sampling은 다음과 같이 크게 두 가지로 나눌 수 있다.\n",
    "* 적은 Class의 Data 수를 증가 시키는 Oversampling\n",
    "* 많은 Class의 Data 수를 감소 시키는 Undersampling\n",
    "\n",
    "## 1. Random Over, Under Sampling\n",
    "- 가장 쉽게 (Over, Under) Sampling 하는 방법은 임의(Random)로 data를 선택하여, 복제하거나 제거하는 방식을 사용할 수 있다.\n",
    "- 하지만, 이러한 방식은 몇가지 문제점이 있다. \n",
    "<br>1)복제하는 경우, 선택된 Data의 위치에 똑같이 점을 찍기 때문에 Data 자체에 과적합될 수 있음\n",
    "<br>2)제거하는 경우, Dataset이 가지고 있는 정보의 손실이 생길 수 있음\n",
    "\n",
    "- Samling Algorithm은 Class 불균형 처리를 위한 imblearn(imbalanced-learn) library에 있다.\n",
    "- Random Over, Under Sampler는 imblearn library의 over_sampling, under_sampling 패키지에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "rus = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2, 3) 데이터에서 특징 찾기 (데이터 비율) +  데이터 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data에서 특징을 학습함과 동시에 Data Sampling\n",
    "# Over Sampling\n",
    "oversampled_data, oversampled_label = ros.fit_resample(data, label)\n",
    "oversampled_data = pd.DataFrame(oversampled_data, columns=data.columns)\n",
    "\n",
    "# Under Sampling\n",
    "undersampled_data, undersampled_label = rus.fit_resample(data, label)\n",
    "undersampled_data = pd.DataFrame(undersampled_data, columns=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('원본 데이터의 클래스 비율 \\n{}'.format(pd.get_dummies(label).sum()))\n",
    "print('\\nRandom Over 샘플링 결과 \\n{}'.format(pd.get_dummies(oversampled_label).sum()))\n",
    "print('\\nRandom Under 샘플링 결과 \\n{}'.format(pd.get_dummies(undersampled_label).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SMOTE(Synthetic Minority Oversampling Technique)\n",
    "- 임의 Over, Under 샘플링은 데이터의 중복으로 인한 과적합 문제와 데이터 손실의 문제가 있었다.\n",
    "- 그런 문제를 최대한 피하면서 데이터를 생성하는 Algorithm인 SMOTE에 대해 알아보자. \n",
    "- SMOTE의 기본 개념은 어렵지 않다. \n",
    "- 수가 적은 Class의 점을 하나 선택해 k개의 가까운 Data Sample을 찾고 그 사이에 새로운 점을 생성한다.\n",
    "- SMOTE의 장점으로는 Data의 손실이 없으며 임의 Over Samping을 하였을 때 보다 과적합을 완화 시킬 수 있다.\n",
    "\n",
    "- 전복 Dataset은 SMOTE로 생성되는 Data sample을 살펴보기 어려우므로, 임의의 Data Sample을 생성해 살펴본다.\n",
    "- 1000개의 Data sample이 5 : 15 : 80 비율로 되어있으며, 2차원 data를 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "data, label = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_repeated=0, n_classes=3,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.05, 0.15, 0.8],\n",
    "                           class_sep=0.8, random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시각화를 통해 생성한 Data를 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(12,6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=label, linewidth=1, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE는 imblearn library의 over_sampling package에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "## k_neighbors parameter로 가까운 Data sample의 수를 결정할 수 있다.\n",
    "smote = SMOTE(k_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2, 3) 데이터에서 특징 찾기 (데이터 비율) +  데이터 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoted_data, smoted_label = smote.fit_resample(data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('원본 데이터의 클래스 비율 \\n{}'.format(pd.get_dummies(label).sum()))\n",
    "print('\\nSMOTE 결과 \\n{}'.format(pd.get_dummies(smoted_label).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.Figure(figsize=(12,6))\n",
    "plt.scatter(smoted_data[:, 0], smoted_data[:, 1], c=smoted_label, linewidth=1, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 2가지 Sampling 방법보다 Data의 분포를 유지하면서 새로운 위치에 Data를 생성할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "## 차원 축소는 해야 하는 이유? - 차원의 저주\n",
    "- 차원의 저주는 저차원에서는 일어나지 않는 현상들이 고차원에서 Data를 분석하거나 다룰 때 생겨나는 현상을 말한다.\n",
    "- 고차원으로 올라갈 수록 공간의 크기가 증가하게 되는데, 데이터는 해당 공간에 한정적으로 위치되어 빈 공간이 많아지기 때문에 발생한다.\n",
    "- 이러한 이유로 Data의 차원이 너무 큰 경우에는 필요없는 변수를 제거하고, 과적합을 방지하기위해 Data의 차원을 축소한다.\n",
    "- 또는, 사람이 인식할 수 있는 차원은 3차원이 최대이므로 Data의 시각화를 위해 차원을 축소하기도 한다.\n",
    "\n",
    "![CurseofDimensionality](./img/Curse_of_Dimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주 성분 분석 (Principal Component Analysis, PCA)\n",
    "- 대표적인 차원 축소 기법으로 주 성분 분석(이하, PCA)이라는 방법이 있다.\n",
    "- PCA는 여러 차원으로 이루어진 Data를 가장 잘 표현하는 축으로 Projection해서 차원을 축소하는 방식을 사용한다.\n",
    "- Data를 가장 잘 표현하는 축이란, Data의 분산을 잘 표현하는 축이라고 할 수 있다.\n",
    "- 기본적으로 주성분(Principal Component, PC)은 Dataset을 특이값 분해를 통해 추출된 고유 Vector이다.\n",
    "- 각 고유 Vector들은 서로 직교성을 띄기 때문에 Data를 주성분로 Projection 시켰을 때 서로 독립적으로 Data를 잘 표현할 수 있다.\n",
    "- PCA의 단점으로는 떨어뜨린 주성분이 어떤 Column인지를 설명할 수 없다는 점이 있다. \n",
    "\n",
    "#### 주 성분 분석의 단계\n",
    "1. 각 컬럼들의 값의 범위를 평균과 표준편차를 사용해 정규화시켜 동일하게 만들어준다. (Scaling)\n",
    "2. Data의 공분산을 계산합니다.\n",
    "3. 공분산 행렬에 대해 특이값 분해를 하여 주성분(고유 vector)과 고유 값을 얻어낸다.\n",
    "4. 주성분과 대응되는 고유 값은 주성분이 Data의 분산을 표현하는 정도의 척도로 사용되므로, 고유 값의 크기와 비율을 보고 몇개의 주성분을 선택할 것인지 또는 원하는 차원의 개수만큼의 주성분을 한다.\n",
    "5. 선택한 주성분으로 모든 Data를 Projection시켜 Data의 차원을 축소한다.\n",
    "\n",
    "#### Projection(사영)\n",
    "- Vector 공간에서 어떤 vector a와 b가 있을 때 vector b를 vector a에 사영한 결과(x)는 아래 그림과 같다.\n",
    "- Vector b를 vector a에 사영한다는 것은 vector a에 대해 수직인 방향으로 vecgor b를 떨어뜨리는 것을 의미한다.\n",
    "- 간단히 말해서, vector b의 그림자를 vector a에 떨어뜨린 것을 생각하면 된다.\n",
    "\n",
    "![Projection](./img/Projection.png)\n",
    "\n",
    "PCA의 기본 원리는 데이터의 분산을 가장 잘 표현하는 vector(축)를 찾아 해당 vector에 Data들을 사영 시키는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 sklearn의 내장 데이터인, 64차원(8\\*8) digit(숫자 image)data를 pca를 통해 2차원으로 떨어뜨려 시각화를 통해 살펴보도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = digits.data\n",
    "label = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자 image가 64 차원 vector로 표현되어 있으므로 image를 확인하기 위해서는 (8,8) 행렬로 변환해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[0].reshape((8,8)))\n",
    "print('Label : {}'.format(label[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0번째 Data는 image 상으로 0으로 보이고, label도 0인 것을 확인하였다.\n",
    "- pca를 통해 64차원 데이터를 2차원 데이터로 차원을 축소 하기로 한다.\n",
    "\n",
    "* 여기에서 digits Data의 각 Pixel(변수)의 Scale은 0 ~ 16으로 같으므로 추가적인 정규화를 하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터에서 특징 찾기 (주 성분 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 변환 (주 성분으로 데이터 사영하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('원본 데이터의 차원 \\n{}'.format(data.shape))\n",
    "print('\\nPCA를 거친 데이터의 차원 \\n{}'.format(new_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_data[:,0], new_data[:, 1], c=label, linewidth=1, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variable to Numeric Variable \n",
    "- 이번에는 범주형 변수를 수치형 변수로 나타내는 방법에 대해 알아보자. \n",
    "- 여기에서 범주형 변수란, 차의 등급을 나타내는 [소형, 중형, 대형] 처럼 표현되는 변수를 말한다. \n",
    "- 범주형 변수는 주로 Data 상에서 문자열로 표현되는 경우가 많으며, 문자와 숫자가 mapping되는 형태로 표현되기도 한다.\n",
    "\n",
    "#### Data Encoding\n",
    "- ML을 위한 대표적인 encoiding 방식은 Label Encoding과 One Hot Encoding이 있다.\n",
    "- Label Encoding은 Category feature를 code형 숫자로 변환하는 것이다.\n",
    "\n",
    "## 1. Label Encoding\n",
    "- Label Encoding은 n개의 범주형 데이터를 0~n-1 의 연속적인 수치 데이터로 표현합니다.\n",
    "- 예를 들어, 차의 등급 변수를 라벨 인코딩으로 변환하면 다음과 같이 표현할 수 있습니다.<br />\n",
    "1)소형 : 0 <br>\n",
    "2)중형 : 1 <br>\n",
    "3)대형 : 2 <br>\n",
    "Label Encoding은 간단한 방법이지만, '소형'과 '중형'이라는 범주형 데이터가 가지고 있는 차이가 0과 1의 수치적인 차이라는 의미가 아님을 주의해야 한다. \n",
    "\n",
    "- Label Encoding과 Sklearn의 preprocessing package에 있다.\n",
    "- 이번에는 전복 Data의 target이었던, 성별  변수를 수치형 변수로 변환하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(abalone_path, header=None, names=abalone_columns)\n",
    "label = data['Sex']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터에서 특징 찾기 (범주의 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 변환 (범주형 변수를 수치형 변수로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoded_label = le.transform(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(data = np.concatenate([label.values.reshape((-1,1)), label_encoded_label.reshape((-1, 1))], axis=1), \n",
    "                      columns=['label', 'label_encoded'])\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.inverse_transform(label_encoded_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다른 예제로 연습해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
    "\n",
    "# LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행. \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print('인코딩 변환값:',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('인코딩 클래스:',encoder.classes_) #많은 경우에 문자열이 어떤 숫자 값으로 변환됐는지 모르기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('디코딩 원본 값:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3])) #다시 디코딩으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-hot Encoding\n",
    "- 이것은 n개의 범주형 데이터를 n개의 비트(0,1) vector로 표현한다.\n",
    "- feature값의 유형에 따라 새로운 feature를 추가하고 고유 값에 해당하는 column에만 1을 표시하고 나머지 column에는 0을 표시하는 방식이다.\n",
    "- 즉, 행 형태로 돼 있는 feature의 고유 값을 열 형태로 차원을 변환한뒤, 고유 값에 해당하는 column에만 1을 표시하고, 나머지 column에는 0을 표시한다.\n",
    "- 예를 들어, 위에서 언급한 소형, 중형, 대형으로 이루어진 범주형 변수를 One-hot Encoding을 통해 변환하면 다음과 같이 표현할 수 있다.<br />\n",
    "소형 : [1, 0, 0] <br>\n",
    "중형 : [0, 1, 0] <br>\n",
    "대형 : [0, 0, 1] <br>\n",
    "- One-hot Encoding으로 범주형 데이터를 나타내게되면, 서로 다른 범주에 대해서는 벡터 내적을 취했을 때 내적 값이 0이 나오게 된다. \n",
    "- 이는 서로 다른 범주 데이터는 독립적인 관계라는 것을 표현할 수 있게 된다.\n",
    "\n",
    "One-hot Encoding은 Sklearn의 preprocessing package에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터에서 특징 찾기 (범주의 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.fit(label.values.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 변환 (범주형 변수를 수치형 변수로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = ohe.transform(label.values.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.concatenate([np.array(['label']) , ohe.categories_[0]])\n",
    "result = pd.DataFrame(data = np.concatenate([label.values.reshape((-1,1)), one_hot_encoded.reshape((-1, 3))], axis=1), \n",
    "                      columns=columns)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 역시 다른 예제로 연습해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
    "\n",
    "# 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환. \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "\n",
    "# 2차원 데이터로 변환. \n",
    "labels = labels.reshape(-1,1)\n",
    "\n",
    "# 원-핫 인코딩을 적용. \n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "print('원-핫 인코딩 데이터')\n",
    "print(oh_labels.toarray())\n",
    "print('원-핫 인코딩 데이터 차원')\n",
    "print(oh_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'item':['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서'] })\n",
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- UCI repository, Abalone DataSet : https://archive.ics.uci.edu/ml/datasets/Abalone \n",
    "- Wikipedia, z-score : https://ko.wikipedia.org/wiki/표준_점수 \n",
    "- Sklearn, Digits datast : https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html&sa=U&ved=0ahUKEwj334uTxODhAhWFgrwKHQBgDd4QFggQMAY&client=internal-uds-cse&cx=016639176250731907682:tjtqbvtvij0&usg=AOvVaw3dwyCabB7mxD5cEn2odXbC\n",
    "- Sklearn, Min-Max Scaler : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html \n",
    "- Sklearn, Standard Scaler : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html \n",
    "- Imblearn, Random OverSampling : https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler \n",
    "- Imblearn, Random UnderSampling : https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler \n",
    "- Imblearn, SMOTE : https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html?highlight=smote\n",
    "- Imblearn, Sampling Examples : https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html?highlight=smote \n",
    "- Curse of Dimension - https://wikidocs.net/7646\n",
    "- Wikipedia, PCA - https://ko.wikipedia.org/wiki/주성분_분석 "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_01) Data Preprocessing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
